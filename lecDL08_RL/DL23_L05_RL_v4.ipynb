{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция 5: Обучение с подкреплением\n",
    "\n",
    "__Автор: Сергей Вячеславович Макрушин__ e-mail: SVMakrushin@fa.ru \n",
    "\n",
    "Финансовый универсиет, 2023 г. \n",
    "\n",
    "При подготовке лекции использованы материалы:\n",
    "* ...\n",
    "\n",
    "v 0.3 01.11.23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разделы: <a class=\"anchor\" id=\"разделы\"></a>\n",
    "* [раздел 1](#загрузка)\n",
    "* [раздел 2](#нормализация)\n",
    "-\n",
    "\n",
    "* [к оглавлению](#разделы)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "﻿<style>\r\n",
       "\r\n",
       "\r\n",
       "b.n {\r\n",
       "    font-weight: normal;        \r\n",
       "}\r\n",
       "\r\n",
       "b.grbg {\r\n",
       "    background-color: #a0a0a0;      \r\n",
       "}\r\n",
       "\r\n",
       "b.r {\r\n",
       "    color: #ff0000;    \r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       "b.b {    \r\n",
       "    color: #0000ff;    \r\n",
       "}\r\n",
       "\r\n",
       "b.g {\r\n",
       "    color: #00ff00;    \r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       "// add your CSS styling here\r\n",
       "\r\n",
       "list-style: none;\r\n",
       "\r\n",
       "ul.s {\r\n",
       "//    list-style-type: none;\r\n",
       "    list-style: none;\r\n",
       "//    background-color: #ff0000;  \r\n",
       "//    color: #ffff00;\r\n",
       "//  padding-left: 1.2em;\r\n",
       "//  text-indent: -1.2em;\r\n",
       "}\r\n",
       "\r\n",
       "li.t {\r\n",
       "    list-style: none;\r\n",
       "//  padding-left: 1.2em;\r\n",
       "//  text-indent: -1.2em;    \r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       "*.r {\r\n",
       "    color: #ff0000;    \r\n",
       "}\r\n",
       "\r\n",
       "li.t:before {\r\n",
       "    content: \"\\21D2\";    \r\n",
       "//    content: \"►\";\r\n",
       "//    padding-left: -1.2em;    \r\n",
       "    text-indent: -1.2em;    \r\n",
       "    display: block;\r\n",
       "    float: left;\r\n",
       "    \r\n",
       "    \r\n",
       "//    width: 1.2em;\r\n",
       "//    color: #ff0000;\r\n",
       "}\r\n",
       "\r\n",
       "i.m:before {\r\n",
       "    font-style: normal;    \r\n",
       "    content: \"\\21D2\";  \r\n",
       "}\r\n",
       "i.m {\r\n",
       "    font-style: normal; \r\n",
       "}    \r\n",
       "\r\n",
       "/*--------------------*/\r\n",
       "/* em {\r\n",
       "    font-style: normal; \r\n",
       "} */\r\n",
       "\r\n",
       "\r\n",
       "em.bl {\r\n",
       "    font-style: normal;     \r\n",
       "    font-weight: bold;        \r\n",
       "}\r\n",
       "\r\n",
       "/* em.grbg {\r\n",
       "    font-style: normal;         \r\n",
       "    background-color: #a0a0a0;      \r\n",
       "} */\r\n",
       "\r\n",
       "em.cr {\r\n",
       "    font-style: normal;         \r\n",
       "    color: #ff0000;    \r\n",
       "}\r\n",
       "\r\n",
       "em.cb {    \r\n",
       "    font-style: normal;         \r\n",
       "    color: #0000ff;    \r\n",
       "}\r\n",
       "\r\n",
       "em.cg {\r\n",
       "    font-style: normal;         \r\n",
       "    color: #00ff00;    \r\n",
       "}\r\n",
       "\r\n",
       "/*--------------------*/\r\n",
       "\r\n",
       "em.qs {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.qs::before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #ff0000;    \r\n",
       "    content: \"Q:\";  \r\n",
       "}\r\n",
       "\r\n",
       "em.an {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.an:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"A:\";  \r\n",
       "}\r\n",
       "    \r\n",
       "em.nt {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.nt:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"Note:\";  \r\n",
       "}    \r\n",
       "    \r\n",
       "em.ex {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.ex:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #00ff00;    \r\n",
       "    content: \"Ex:\";  \r\n",
       "} \r\n",
       "    \r\n",
       "em.df {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.df:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"Def:\";  \r\n",
       "}    \r\n",
       "\r\n",
       "em.pl {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.pl:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"+\";  \r\n",
       "}    \r\n",
       "\r\n",
       "em.mn {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.mn:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"-\";  \r\n",
       "}        \r\n",
       "\r\n",
       "em.plmn {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.plmn:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"\\00B1\";\\\\\"&plusmn;\";  \r\n",
       "}\r\n",
       "    \r\n",
       "em.hn {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.hn:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"\\21D2\";\\\\\"&rArr;\";  \r\n",
       "}     \r\n",
       "    \r\n",
       "\r\n",
       "#cssTableCenter td, th \r\n",
       "{\r\n",
       "    text-align: center; \r\n",
       "    vertical-align: middle;\r\n",
       "}\r\n",
       "\r\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загружаем стиль для оформления презентации\n",
    "from IPython.display import HTML\n",
    "from urllib.request import urlopen\n",
    "html = urlopen(\"file:./lec_v2.css\")\n",
    "HTML(html.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение в обучение с подкреплением"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Различные подходы к машинному обучению__\n",
    "\n",
    "<br/>\n",
    "<center>         \n",
    "    <img src=\"./img/l_types1.png\" alt=\"\" style=\"width: 700px;\"/>\n",
    "    <b>Типы машинного обучения (Часть 1)</b> <br/>\n",
    "</center>\n",
    "\n",
    "__Обучение с учителем__ (Supervised Learning) - модель обучается __на размеченных данных__, где каждый входной пример имеет соответствующий целевой выход.\n",
    "* __Цель__ состоит в том, чтобы модель научилась __предсказывать целевой выход для новых__, ранее не виденных __данных__.\n",
    "* __Задачи__: классификация и регрессия.\n",
    "\n",
    "__Обучение без учителя__ (Unsupervised Learning) - модель обучается на неразмеченных данных, где отсутствует целевой выход.\n",
    "* __Цель__ - найти внутренние структуры, закономерности или группировки в данных.\n",
    "* __Задачи__: кластеризация, выявление аномалий, понижение размерности, поиск ассоциативных правил.\n",
    "\n",
    "__Обучение с частичным привлечением учителя__ (Semi-Supervised Learning) - комбинация обучения с учителем и обучения без учителя, где модель обучается на __небольшом наборе размеченных данных__ и __большем наборе неразмеченных данных__.\n",
    "* __Цель и задачи__ аналогичны обучению с учителем, __идея__ заключается в использовании неразмеченных данных для улучшения обучения модели и расширения ее способности к обобщению.\n",
    "\n",
    "__Самообучение__ (Self-Supervised Learning) - подходк к обучению, в котором модель обучается на неразмеченных данных, используя разнообразные задачи, создаваемые из самих данных, без необходимости внешних разметок. Вместо того, чтобы полагаться на учителя для предоставления меток (как в обучении с учителем), Self-Supervised Learning использует информацию, извлекаемую из данных, для самостоятельного обучения модели.\n",
    "* __Цель и задачи__ аналогичны обучению с учителем, входые данные - аналогичны обучению без учителя. (см.: https://dasha.ai/en-us/blog/self-supervised-machine-learning )\n",
    "\n",
    "\n",
    "<br/>\n",
    "<center>         \n",
    "    <img src=\"./img/act_learning.png\" alt=\"\" style=\"width: 350px;\"/>\n",
    "    <b>Принцип работы активного обучения</b> <br/>\n",
    "</center>\n",
    "\n",
    "__Активное обучение (Active Learning)__ - метод, применяемый в подходе __обучения с учителем__, обычно в случаях __когда получение меток дорого__, поэтому мы получаем __новые метки динамически__, определяя алгоритмическую __стратегию для максимизации полезности новых наблюдений__. \n",
    "* Можно рассматривать активное обучение как __особый случай обучения с частичным привлечением учителя__.\n",
    "* __Цель и задачи__ аналогичны обучению с учителем, входые данные - без разметки, с возможностью (обычно дорогстоящей) динамической разметки.\n",
    "\n",
    "<br/>\n",
    "<center>         \n",
    "    <img src=\"./img/rl1.png\" alt=\"\" style=\"width: 500px;\"/>\n",
    "    <b>Принцип работы обучения с подкреплением</b> <br/>\n",
    "</center>\n",
    "\n",
    "__Обучение с подкреплением (Reinforcement Learning, RL)__ - модель обучается как __агент, взаимодействующий с окружающей средой__ и максимизируют вознаграждение получаемое от системы.\n",
    "* В RL агент самостоятельно исследует окружающую среду, принимая решения на основе своего опыта и обратной связи от среды. \n",
    "* В RL нет меток:\n",
    "    * нельзя использовать обучение с учителем\n",
    "    * есть \"вознаграждение\", которое говорит насколько хорош текущий результат.Следовательно,\n",
    "* В RL модель __учится стратегии__ максимизации вознаграждения.\n",
    "    * стратегия должна должна учитывать баланс между исследованием (получением новый информации о среде) и использованием существующих знаний о ней для максимизации вознаграждения.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Приложения обученяи с подкреплением__\n",
    "\n",
    "<br/>\n",
    "<center>         \n",
    "    <img src=\"./img/rl_ap1.png\" alt=\"\" style=\"width: 650px;\"/>\n",
    "    <b>Приложения обученяи с подкреплением</b> <br/>\n",
    "</center>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Постановка задачи обучения с подкреплением__\n",
    "\n",
    "__Обучение с подкреплением__ (reinforcement learning) — метод машинного обучения, при котором система обучается, взаимодействуя с некоторой средой. В обучении с подкреплением есть:\n",
    "* __агент (agent)__ кторый при помощи\n",
    "* __действий (actions)__ взаимодействует с \n",
    "* окружающей __средой (environment)__ описываемой\n",
    "* внутренним __состоянием (state)__\n",
    "* в ответ на действие среда возвращает __вознаграждение (reward)__ за эти действия\n",
    "* и меняет свое сотоятение.\n",
    "\n",
    "<br/>\n",
    "<center>         \n",
    "    <img src=\"./img/rl1.png\" alt=\"\" style=\"width: 500px;\"/>\n",
    "    <b>Принцип работы обучения с подкреплением</b> <br/>\n",
    "</center>\n",
    "\n",
    "* $s_t$ - сотояние среды (state) на шаге $t$\n",
    "* $a_t$ - действия агента (agent) на шаге $t$\n",
    "* $r_t$ - вознаграждение (reward) на шаге $t$\n",
    "\n",
    "---\n",
    "* $s \\in S$ - сотояния\n",
    "* $a \\in A$ - действия\n",
    "* $p(s'|a, s)$ - модель переходов между состояниями (в общем случае - вероятностная)\n",
    "    * модель переходов неизвестна агенту и в зависимости от подхода к решению задачи агент может пытаться, а может не пытаться узнать модель переходов.\n",
    "* $\\pi(a|s)$ - стратегия (policy, политика) \n",
    "* $r(s, a)$ - вознаграждение\n",
    "    * для многих задач построение функции вознаграждения сложная задача. Пример: шахматы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Стратегия в RL__\n",
    "\n",
    "__Стратегия__ (policy, политика) в RL - это стратегия или набор правил, которые определяют, какое действие должен выбирать агент должен при заданном состоянии среды.\n",
    "* Формально может быть записано как распределение: $\\pi(a|s)$\n",
    "* Статегия определяет, как агент будет взаимодействовать с окружающей средой, и важна для достижения целей обучения с подкреплением.\n",
    "\n",
    "Стратегия может быть определена различными способами:\n",
    "* __Детерминированная стратегия__ -  для каждого состояния среды существует конкретное действие, которое агент должен выполнить. \n",
    "    * Например, если агент находится в состоянии A, он всегда выполняет действие X.\n",
    "* __Случайная стратегия__ - агент выбирает действия с определенными вероятностями для каждого состояния.\n",
    "* __Параметризованная стратегия__ - например, стратегия может быть параметризованной с использованием нейронных сетей, где параметры модели настраиваются в процессе обучения. В записи $\\pi(a|s,\\mathbf{\\theta})$ $\\mathbf{\\theta}$ это векторо параметров модели, например веса нейронной сети.\n",
    "\n",
    "\n",
    "* __Цель обучения__ с подкреплением заключается в __нахождении оптимальной стратегии__, которая максимизирует ожидаемое вознаграждение (reward) в долгосрочной перспективе. \n",
    "* __Агент обучается, итеративно взаимодействуя с средой и обновляя свою стратегию__, чтобы лучше адаптироваться к окружающей среде и достигать своих целей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Определение вознаграждения__\n",
    "\n",
    "* Траектория агента: ${(s_0, a_0), (s_1, a_1), \\ldots}$ \n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/mp1.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "    <b>It is a special case of a Markov Decision Process (MDP): A graph where each node is a particular game state and each edge is a possible (in general probabilistic) transition. Each edge also gives a reward, and the goal is to compute the optimal way of acting in any state to maximize rewards.</b> <br/>\n",
    "</center>\n",
    "\n",
    "* Суммарное дисконтированное вознаграждение тогда: \n",
    "$$R_t = \\gamma^0 r(s_t, a_t) + \\gamma^1 r(s_{t+1}, a_{t+1}) + \\gamma^2 r(s_{t+2}, a_{t+2}) + \\ldots$$\n",
    "где $\\gamma \\in [0, 1]$ - дисконтирующий множитель\n",
    "\n",
    "$$R_0 = \\sum_{t=0}^{\\infty} \\gamma^t r_t$$\n",
    "\n",
    "где:\n",
    "- $R$ - дисконтированное вознаграждение.\n",
    "- $t$ - шаг времени.\n",
    "- $\\gamma$ - коэффициент дисконтирования, обычно $\\gamma \\in [0, 1]$.\n",
    "- $r_t$ - награда (reward) на шаге времени \\(t\\).\n",
    "\n",
    "Это выражение представляет собой бесконечную сумму наград, взвешенных коэффициентами дисконтирования \\(\\gamma^t\\), где \\(\\gamma\\) определяет, как сильно будут учитываться будущие награды по сравнению с текущей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Функции V и Q__\n",
    "\n",
    "__Функция значения состояния__ (value function, $V^\\pi(s)$) - ожидаемая сумма дисконтированных будущих вознаграждений при следовании политике \\(\\pi\\) если начать из состоянии $s$:\n",
    "\n",
    "$$V^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_t \\,|\\, s_0 = s \\right]$$\n",
    "\n",
    "Где:\n",
    "* $\\mathbb{E}_\\pi$ - оператор математического ожидания по политике $\\pi$.\n",
    "* $r_t$ - награда (reward) на временном шаге $t$.\n",
    "* $s_0$ - начальное состояние.\n",
    "\n",
    "__Q-функция__ - ожидаемая сумма дисконтированных будущих вознаграждений при следовании политике \\(\\pi\\) если начать из состоянии $s$ при выполнении действия $a$:\n",
    "\n",
    "$$Q^\\pi(s, a) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_t \\,|\\, s_0 = s, a_0 = a \\right]$$\n",
    "\n",
    "* Если бы мы знали функции $V$ и $Q$ то можно было бы просто выбирать то действие $a$, которое максимизирует $Q(s,a)$.\n",
    "* Т.е. функции $V$ и $Q$ — это как раз то, что нам нужно оценить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Уравнение Беллмана для функции значения состояния__\n",
    "\n",
    "$$V^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_t \\,|\\, s_0 = s \\right]=$$\n",
    "\n",
    "первый член - это награда $r_0$, которая получается после выполнения действия $a_0$ в состоянии $s_0$:\n",
    "\n",
    "$$= \\mathbb{E}_\\pi \\left[ r_0  + \\gamma \\sum_{t=0}^{\\infty} \\gamma^t r_{t+1} \\,|\\, s_0 = s \\right]$$\n",
    "\n",
    "распишем матожидание при выполнении первого шага стратегии:\n",
    "\n",
    "$$V^\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s' | s, a) \\left[ R(s, a, s') + \\gamma \\mathbb{E}_\\pi \\left[\\sum_{t=0}^{\\infty} \\gamma^t r_{t+1} \\,|\\, s_0 = s' \\right] \\right] $$\n",
    "\n",
    "выражение во вторых квадратных скобках это функция значения состояния для нового состояния $s'$\n",
    "\n",
    "$$= \\sum_a \\pi(a|s) \\sum_{s'} P(s' | s, a) [R(s, a, s') + \\gamma V^\\pi(s')]$$\n",
    "\n",
    "Т.е. получаем рекурентную зависимость которая является уравнением Беллмана:\n",
    "\n",
    "$$V^\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s' | s, a) [R(s, a, s') + \\gamma V^\\pi(s')]$$\n",
    "\n",
    "__Уравнение Беллмана__ играет важную роль в динамическом программировании: оно позволяет __рекурсивно вычислять оптимальные значения для всех состояний на основе оптимальных значений для более коротких траекторий__.\n",
    "* Это ключевой элемент для поиска оптимальных стратегий в марковских процессах принятия решений и обучении с подкреплением. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уравнение Беллмана для функции значения состояния $V(s)$ выглядит следующим образом:\n",
    "\n",
    "Пусть функция значения состояния при оптимальной стратегии: $V^{*}(s) = \\max_{\\pi} \\mathbb{E} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_t \\right]$, тогда:\n",
    "\n",
    "$$V^{*}(s) = \\max_a \\sum_{s'} P(s' | s, a) [R(s, a, s') + \\gamma V^{*}(s')]$$\n",
    "\n",
    "Где:\n",
    "* $V(s)$ - значение состояния $s$, то есть ожидаемая сумма наград при следовании оптимальной стратегии из состояния $s$.\n",
    "* $a$ - действие, выбранное в состоянии $s$ в соответствии с оптимальной стратегией.\n",
    "* $s'$ - следующее состояние.\n",
    "* $P(s' | s, a)$ - вероятность перехода из состояния $s$ в состояние $s'$ при выполнении действия $a$.\n",
    "* $R(s, a, s')$ - вознаграждение (reward), получаемая после выполнения действия $a$ в состоянии $s$ и перехода в состояние $s'$.\n",
    "* $\\gamma$ - коэффициент дисконтирования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Два подхода к нахождению оптимальной стратегии__\n",
    "\n",
    "Tеоретически для того чтобы найти значения $V(s)$, можно просто решить систему линейных уравнений, неизвестными в которых являются $V(s)$ для разных состояний $s$.\n",
    "\n",
    "Но __для этого нужно знать все параметры марковского процесса__, то есть функции R и P, а с этим в реальных ситуациях все сложно. \n",
    "* Все, что у нас обычно есть на входе — __окружающая среда, выдающая вознаграждения как черный ящик__. Т.е. в реальных задачах функции $R$ и $P$ тоже приходится обучать.\n",
    "\n",
    "Методы обучения с подкреплением делятся на те, которые:\n",
    "* обучают функции $R$ и $P$ в явном виде\n",
    "* обходятся без этого и сразу обучают $V$ и/или $Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Классификация алгоритмов RL__\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/rl_alg.png\" alt=\"\" style=\"width: 700px;\"/>\n",
    "    <b>Классификация алгоритмов RL</b> <br/>\n",
    "</center>\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/rl2.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "    <b>Подходы к RL</b> <br/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Введение в метод Policy Gradient__\n",
    "\n",
    "__Policy gradient__ Класс методов reinforcement learning в которых стратегию $\\pi_{\\theta}(a|s)$ оптимизируют напрямую (в отличие от __Q-learning__ ). \n",
    "\n",
    "Интуитивное объяснение принципа работы:\n",
    "\n",
    "$p_{\\theta}(\\tau)$ - вероятность того, что будет реализован сценарий $\\tau$ при условии параметров модели $\\theta$, т. е. функция правдоподобия. Нам хочется:\n",
    "* увеличить правдоподобие \"хороших\" сценариев (обладающих высоким $R_{\\tau}$)\n",
    "* понизить правдоподобие \"плохих\" сценариев (с низким $R_{\\tau}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Игра Pong__\n",
    "\n",
    "Игра Pong: https://www.youtube.com/watch?v=fiShX2pTz9A \n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/pong1.png\" alt=\"\" style=\"width: 500px;\"/>\n",
    "    <b>Нейронная сеть для обучения игре Pong</b> <br/>\n",
    "</center>\n",
    "\n",
    "* На нейроны подается информация __о разности__ значений пикселей в последующих кадрах игры.\n",
    "* Выходной нейрон - вероятность двигать ракетку вверх (иначе двигаем ракетку вниз)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Принцип обучения методом Policy Gradient__\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/pg1.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "    <b>Процесс обучения при обучении с учителем</b> <br/>\n",
    "</center>\n",
    "\n",
    "При обучении с учителем на каждом шаге мы знаем правильный ответ (правильный класс: \"вверх\" или \"вниз\"), на основании которого и проводим обратное распространение ошибки.\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/pg2.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "    <b>Процесс обучения в Policy Gradient</b> <br/>\n",
    "</center>\n",
    "\n",
    "* У нас нет правильного ответа для каждого шага игры, есть только ответ для всей игры: \"выиграл\" или \"проиграл\".\n",
    "* __В случае выигрыша__ действие на каждом шаге (выбранный класс) поощряется: устанавливается что оно было верным.\n",
    "* __В случае проигрыша__ действие на каждом шаге (выбранный класс) штрафуется: устанавливается что оно было НЕ верным."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Пример сценариев обучения в Policy Gradient__\n",
    "\n",
    "* __Каждая точка__ представляет собой некоторое игровое __состояние__ (три примера состояния визуализированы внизу)\n",
    "* __Каждая стрелка__ - это переход (шаг по времени): действие (указано у стрелки) и изменение состояния.\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/pg3.png\" alt=\"\" style=\"width: 700px;\"/>\n",
    "    <b>Пример 4 тректорий игры в Pong</b> <br/>\n",
    "</center>\n",
    "\n",
    "* В этих примерах мы выиграли 2 игры и проиграли 2 игры. \n",
    "\n",
    "Используя Policy Gradients:\n",
    "* мы поощряем две выигранные игры: слегка поощряем каждое действие, которое мы совершили в этом эпизоде. \n",
    "* мы штрафуем  две проигранные игры: слегка штрафуем каждое действие, которое мы совершили в этом эпизоде.\n",
    "\n",
    "\n",
    "* Pong AI with Policy Gradients https://www.youtube.com/watch?v=YOW8m2YGtRg&list=TLGG2Hi1VyMnZ7gwMjExMjAyMw\n",
    "    * The learned agent (in green, right) facing off with the hard-coded AI opponent (left)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Loss Policy Gradients__\n",
    "\n",
    "Обозначения:\n",
    "* $p(x)=p(x|s,\\theta)$ - policy function, $\\theta$ - веса сети\n",
    "* $f(x)$ - вознаграждения \n",
    "\n",
    "По\n",
    "<center>         \n",
    "    <img src=\"./img/pg4.png\" alt=\"\" style=\"width: 700px;\"/>\n",
    "    <b>...</b> <br/>\n",
    "</center>\n",
    "\n",
    "* Что весьма похоже на Cross-entropy loss\n",
    "\n",
    "__Алгоритм обучения с помощью Policy Gradients__\n",
    "1. Разыгрываем N игр при помощи имеющейся нейросети реализующей стратегию $\\pi(a|s)$ - получаем сценарии игр для обучения.\n",
    "2. На основе Loss Policy Gradients по полученным сценариям игр обучаем нейросеть стратегии.\n",
    "3. Меняем нейросеть стратегии $\\pi(a|s)$  на обученную, переходим к п.1 \n",
    "\n",
    "\n",
    "Ссылка: http://karpathy.github.io/2016/05/31/rl/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>         \n",
    "    <img src=\"./img/pg5.png\" alt=\"\" style=\"width: 700px;\"/>\n",
    "    <b>...</b> <br/>\n",
    "</center>\n",
    "<br/>\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/pg6.png\" alt=\"\" style=\"width: 700px;\"/>\n",
    "    <b>...</b> <br/>\n",
    "</center>\n",
    "<br/>\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/pg7.png\" alt=\"\" style=\"width: 700px;\"/>\n",
    "    <b>Baseline</b> <br/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Преимущества Policy gradient:__\n",
    "\n",
    "* __Легко обобщается на задачи с большим множеством действий___, в том числе на задачи с непрерывным множеством действий;\n",
    "* По большей части __избегает конфликта между эксплуатацией (exploitation) и исследованием (exploration)__, так как оптимизирует напрямую стохастическую стратегию $\\pi_{\\theta}(a|s)$;\n",
    "* Имеет __более сильные гарантии сходимости__: если Q-learning гарантированно сходится только для МППР с конечными множествами действий и состояний, то policy gradient сходится к локальному оптимуму всегда, в том числе в случае бесконечных множеств действий и состояний.\n",
    "\n",
    "__Недостатки Policy gradient:__\n",
    "\n",
    "* __Очень низкая скорость работы__ - требуется большое количество вычислений для оценки $\\nabla_{\\theta} J(\\theta)$ по методу Монте-Карло, так как:\n",
    "     * для получения всего одного семпла требуется произвести $T$ взаимодействий со средой;\n",
    "     * случайная величина $\\nabla_{\\theta} \\log p_{\\theta}(\\tau) R_{\\tau}$ имеет большую дисперсию поэтому для точной оценки $\\nabla_{\\theta} J(\\theta) = E_{\\tau \\sim p_{\\theta}(\\tau)} \\left[ \\nabla_{\\theta} \\log p_{\\theta}(\\tau) R_{\\tau} \\right]$ требуется много семплов;\n",
    "    * __cемплы__, собранные для предыдущих значений $\\theta$, __никак не переиспользуются на следующем шаге__, семплирование нужно делать заново на каждом шаге градиентного спуска.\n",
    "* В случае конечных марковских процессов принятия решений (МППР) Q-learning сходится к глобальному оптимуму, тогда как __policy gradient может застрять в локальном__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В модели Actor-Critic обучается функции значения состояния $b=V(s)$ предсказывающая дисконтированное вознаграждение по текущему состоянию:\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/ac2.png\" alt=\"\" style=\"width: 700px;\"/>\n",
    "    <b>Baseline</b> <br/>\n",
    "</center>\n",
    "\n",
    "Обучение $b=V(s)$ проводится параллельно собучением Policy gradient рассматривающей не исходое вознаграждение а дельту вознаграждения ( $\\log p(a) \\sum_i \\left( \\gamma^i r_{s+i} - V\\right)$ ) и ожидаемого значения состояния для текущего состояния:\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/ac1.png\" alt=\"\" style=\"width: 700px;\"/>\n",
    "    <b>Baseline</b> <br/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Уравнение Беллмана для функции значения состояния__\n",
    "\n",
    "Уравнение Беллмана для функции значения состояния $V(s)$ выглядит следующим образом:\n",
    "\n",
    "Пусть функция значения состояния при оптимальной стратегии: $V^{*}(s) = \\max_{\\pi} \\mathbb{E} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_t \\right]$, тогда:\n",
    "\n",
    "$$V^{*}(s) = \\max_a \\sum_{s'} P(s' | s, a) [R(s, a, s') + \\gamma V^{*}(s')]$$\n",
    "\n",
    "Где:\n",
    "* $V(s)$ - значение состояния $s$, то есть ожидаемая сумма наград при следовании оптимальной стратегии из состояния $s$.\n",
    "* $a$ - действие, выбранное в состоянии $s$ в соответствии с оптимальной стратегией.\n",
    "* $s'$ - следующее состояние.\n",
    "* $P(s' | s, a)$ - вероятность перехода из состояния $s$ в состояние $s'$ при выполнении действия $a$.\n",
    "* $R(s, a, s')$ - вознаграждение (reward), получаемая после выполнения действия $a$ в состоянии $s$ и перехода в состояние $s'$.\n",
    "* $\\gamma$ - коэффициент дисконтирования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Уравенение Беллмана для функции Q__\n",
    "\n",
    "__Q-функция__ - ожидаемая сумма дисконтированных будущих вознаграждений при следовании политике \\(\\pi\\) если начать из состоянии $s$ при выполнении действия $a$:\n",
    "\n",
    "$$Q^\\pi(s, a) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_t \\,|\\, s_0 = s, a_0 = a \\right]$$\n",
    "\n",
    "* Если бы мы знали функции $V$ и $Q$ то можно было бы просто выбирать то действие $a$, которое максимизирует $Q(s,a)$.\n",
    "* Т.е. функции $V$ и $Q$ — это как раз то, что нам нужно оценить.\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/ql1.png\" alt=\"\" style=\"width: 500px;\"/>\n",
    "    <b>___</b> <br/>\n",
    "</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Алгоритм DQL__\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/ql3.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "    <b>Схема нейросети Q для игр на Atari</b> <br/>\n",
    "</center>\n",
    "\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/ql2.png\" alt=\"\" style=\"width: 700px;\"/>\n",
    "    <b>___</b> <br/>\n",
    "</center>\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/ql4.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "    <b>Работа с памятью эпизодов</b> <br/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Сравнение подходов__\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/comp1.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "    <b>Сравнение подходов</b> <br/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Спасибо за внимание!\n",
    "\n",
    "---\n",
    "### Технический раздел:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://neerc.ifmo.ru/wiki/index.php?title=Методы_policy_gradient_и_алгоритм_асинхронного_актора-критика\n",
    "* https://neerc.ifmo.ru/wiki/index.php?title=Обучение_с_подкреплением"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "методы, которые позволяют оптимизировать стратегию $\\pi_{\\theta}(s|a)$ напрямую. Такие алгоритмы относятся к классу алгоритмов ''policy gradient''.\n",
    "\n",
    "Интуитивное объяснение принципа работы:\n",
    "\n",
    "$p_{\\theta}(\\tau)$ - вероятность того, что будет реализован сценарий $\\tau$ при условии параметров модели $\\theta$, т. е. функция правдоподобия. Нам хочется увеличить правдоподобие \"хороших\" сценариев (обладающих высоким $R_{\\tau}$) и понизить правдоподобие \"плохих\" сценариев (с низким $R_{\\tau}$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Policy gradient__ Класс методов reinforcement learning в которых стратегию $\\pi_{\\theta}(s|a)$ оптимизируют напрямую (в отличие от _Q-learning_ ). \n",
    "\n",
    "Интуитивное объяснение принципа работы:\n",
    "\n",
    "$p_{\\theta}(\\tau)$ - вероятность того, что будет реализован сценарий $\\tau$ при условии параметров модели $\\theta$, т. е. функция правдоподобия. Нам хочется увеличить правдоподобие \"хороших\" сценариев (обладающих высоким $R_{\\tau}$) и понизить правдоподобие \"плохих\" сценариев (с низким $R_{\\tau}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Двигаясь вверх по градиенту функции полного выигрыша мы повышаем логарифм функции правдоподобия для сценариев, имеющих больший положительный $R_{\\tau}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преимущества Policy gradient:\n",
    "\n",
    "* Легко обобщается на задачи с большим множеством действий, в том числе на задачи с непрерывным множеством действий;\n",
    "* По большей части избегает конфликта между эксплуатацией (exploitation) и исследованием (exploration), так как оптимизирует напрямую стохастическую стратегию $\\pi_{\\theta}(a|s)$;\n",
    "* Имеет более сильные гарантии сходимости: если Q-learning гарантированно сходится только для МППР с конечными множествами действий и состояний, то policy gradient сходится к локальному оптимуму всегда, в том числе в случае бесконечных множеств действий и состояний.\n",
    "\n",
    "Недостатки Policy gradient:\n",
    "\n",
    "* Очень низкая скорость работы - требуется большое количество вычислений для оценки $\\nabla_{\\theta} J(\\theta)$ по методу Монте-Карло, так как:\n",
    "     * для получения всего одного семпла требуется произвести $T$ взаимодействий со средой;\n",
    "     * случайная величина $\\nabla_{\\theta} \\log p_{\\theta}(\\tau) R_{\\tau}$ имеет большую дисперсию поэтому для точной оценки $\\nabla_{\\theta} J(\\theta) = E_{\\tau \\sim p_{\\theta}(\\tau)} \\left[ \\nabla_{\\theta} \\log p_{\\theta}(\\tau) R_{\\tau} \\right]$ требуется много семплов;\n",
    "    * cемплы, собранные для предыдущих значений $\\theta$, никак не переиспользуются на следующем шаге, семплирование нужно делать заново на каждом шаге градиентного спуска.\n",
    "* В случае конечных марковских процессов принятия решений (МППР) Q-learning сходится к глобальному оптимуму, тогда как policy gradient может застрять в локальном."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> next <em class=\"qs\"></em> qs line \n",
    "<br/> next <em class=\"an\"></em> an line \n",
    "<br/> next <em class=\"nt\"></em> an line \n",
    "<br/> next <em class=\"df\"></em> df line \n",
    "<br/> next <em class=\"ex\"></em> ex line \n",
    "<br/> next <em class=\"pl\"></em> pl line \n",
    "<br/> next <em class=\"mn\"></em> mn line \n",
    "<br/> next <em class=\"plmn\"></em> plmn line \n",
    "<br/> next <em class=\"hn\"></em> hn line "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
